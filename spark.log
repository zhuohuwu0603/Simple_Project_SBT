[INFO] 2016-04-06 19:53:21,429 org.apache.spark.SecurityManager logInfo - Changing view acls to: zhuohuawu,
[INFO] 2016-04-06 19:53:21,431 org.apache.spark.SecurityManager logInfo - Changing modify acls to: zhuohuawu,
[INFO] 2016-04-06 19:53:21,433 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zhuohuawu, ); users with modify permissions: Set(zhuohuawu, )
[INFO] 2016-04-06 19:53:21,744 akka.event.slf4j.Slf4jLogger applyOrElse - Slf4jLogger started
[INFO] 2016-04-06 19:53:21,778 Remoting apply$mcV$sp - Starting remoting
[INFO] 2016-04-06 19:53:21,889 Remoting apply$mcV$sp - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.142:55641]
[INFO] 2016-04-06 19:53:21,890 Remoting apply$mcV$sp - Remoting now listens on addresses: [akka.tcp://sparkDriver@192.168.1.142:55641]
[INFO] 2016-04-06 19:53:21,895 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 55641.
[INFO] 2016-04-06 19:53:21,909 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2016-04-06 19:53:21,913 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2016-04-06 19:53:21,925 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /var/folders/0y/3r3b4yz11pqfyv3xh382szlh0000gn/T/spark-local-20160406195321-9d35
[INFO] 2016-04-06 19:53:21,945 org.apache.spark.util.Utils logInfo - Successfully started service 'Connection manager for block manager' on port 55642.
[INFO] 2016-04-06 19:53:21,946 org.apache.spark.network.ConnectionManager logInfo - Bound socket to port 55642 with id = ConnectionManagerId(192.168.1.142,55642)
[INFO] 2016-04-06 19:53:21,949 org.apache.spark.storage.MemoryStore logInfo - MemoryStore started with capacity 1966.1 MB
[INFO] 2016-04-06 19:53:21,961 org.apache.spark.storage.BlockManagerMaster logInfo - Trying to register BlockManager
[INFO] 2016-04-06 19:53:21,962 org.apache.spark.storage.BlockManagerMasterActor logInfo - Registering block manager 192.168.1.142:55642 with 1966.1 MB RAM
[INFO] 2016-04-06 19:53:21,965 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager
[INFO] 2016-04-06 19:53:21,976 org.apache.spark.HttpFileServer logInfo - HTTP File server directory is /var/folders/0y/3r3b4yz11pqfyv3xh382szlh0000gn/T/spark-7a6e743d-a6ef-40c0-91d9-a37547340494
[INFO] 2016-04-06 19:53:21,980 org.apache.spark.HttpServer logInfo - Starting HTTP Server
[INFO] 2016-04-06 19:53:22,049 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[INFO] 2016-04-06 19:53:22,061 org.eclipse.jetty.server.AbstractConnector doStart - Started SocketConnector@0.0.0.0:55643
[INFO] 2016-04-06 19:53:22,061 org.apache.spark.util.Utils logInfo - Successfully started service 'HTTP file server' on port 55643.
[INFO] 2016-04-06 19:53:22,231 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[INFO] 2016-04-06 19:53:22,239 org.eclipse.jetty.server.AbstractConnector doStart - Started SelectChannelConnector@0.0.0.0:4040
[INFO] 2016-04-06 19:53:22,239 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2016-04-06 19:53:22,241 org.apache.spark.ui.SparkUI logInfo - Started SparkUI at http://192.168.1.142:4040
[INFO] 2016-04-06 19:53:22,561 org.apache.spark.util.AkkaUtils logInfo - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.1.142:55641/user/HeartbeatReceiver
[INFO] 2016-04-06 19:53:22,644 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(32768) called with curMem=0, maxMem=2061647216
[INFO] 2016-04-06 19:53:22,646 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 32.0 KB, free 1966.1 MB)
[WARN] 2016-04-06 19:53:22,710 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN] 2016-04-06 19:53:22,711 org.apache.hadoop.io.compress.snappy.LoadSnappy <clinit> - Snappy native library not loaded
[INFO] 2016-04-06 19:53:22,716 org.apache.hadoop.mapred.FileInputFormat listStatus - Total input paths to process : 1
[INFO] 2016-04-06 19:53:22,725 org.apache.spark.SparkContext logInfo - Starting job: count at SimpleApp.scala:22
[INFO] 2016-04-06 19:53:22,735 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (count at SimpleApp.scala:22) with 2 output partitions (allowLocal=false)
[INFO] 2016-04-06 19:53:22,736 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: Stage 0(count at SimpleApp.scala:22)
[INFO] 2016-04-06 19:53:22,736 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2016-04-06 19:53:22,739 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2016-04-06 19:53:22,745 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting Stage 0 (FilteredRDD[2] at filter at SimpleApp.scala:22), which has no missing parents
[INFO] 2016-04-06 19:53:22,769 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(2584) called with curMem=32768, maxMem=2061647216
[INFO] 2016-04-06 19:53:22,769 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 2.5 KB, free 1966.1 MB)
[INFO] 2016-04-06 19:53:22,779 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 2 missing tasks from Stage 0 (FilteredRDD[2] at filter at SimpleApp.scala:22)
[INFO] 2016-04-06 19:53:22,780 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 2 tasks
[INFO] 2016-04-06 19:53:22,795 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1213 bytes)
[INFO] 2016-04-06 19:53:22,798 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1213 bytes)
[INFO] 2016-04-06 19:53:22,801 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2016-04-06 19:53:22,801 org.apache.spark.executor.Executor logInfo - Running task 1.0 in stage 0.0 (TID 1)
[INFO] 2016-04-06 19:53:22,819 org.apache.spark.CacheManager logInfo - Partition rdd_1_1 not found, computing it
[INFO] 2016-04-06 19:53:22,819 org.apache.spark.CacheManager logInfo - Partition rdd_1_0 not found, computing it
[INFO] 2016-04-06 19:53:22,820 org.apache.spark.rdd.HadoopRDD logInfo - Input split: file:/Users/zhuohuawu/Documents/zw_codes/spark/README.md:1820+1820
[INFO] 2016-04-06 19:53:22,820 org.apache.spark.rdd.HadoopRDD logInfo - Input split: file:/Users/zhuohuawu/Documents/zw_codes/spark/README.md:0+1820
[INFO] 2016-04-06 19:53:22,841 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(5400) called with curMem=35352, maxMem=2061647216
[INFO] 2016-04-06 19:53:22,841 org.apache.spark.storage.MemoryStore logInfo - Block rdd_1_1 stored as values in memory (estimated size 5.3 KB, free 1966.1 MB)
[INFO] 2016-04-06 19:53:22,843 org.apache.spark.storage.BlockManagerInfo logInfo - Added rdd_1_1 in memory on 192.168.1.142:55642 (size: 5.3 KB, free: 1966.1 MB)
[INFO] 2016-04-06 19:53:22,843 org.apache.spark.storage.BlockManagerMaster logInfo - Updated info of block rdd_1_1
[INFO] 2016-04-06 19:53:22,845 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(6232) called with curMem=40752, maxMem=2061647216
[INFO] 2016-04-06 19:53:22,845 org.apache.spark.storage.MemoryStore logInfo - Block rdd_1_0 stored as values in memory (estimated size 6.1 KB, free 1966.1 MB)
[INFO] 2016-04-06 19:53:22,845 org.apache.spark.storage.BlockManagerInfo logInfo - Added rdd_1_0 in memory on 192.168.1.142:55642 (size: 6.1 KB, free: 1966.1 MB)
[INFO] 2016-04-06 19:53:22,846 org.apache.spark.storage.BlockManagerMaster logInfo - Updated info of block rdd_1_0
[INFO] 2016-04-06 19:53:22,860 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 2300 bytes result sent to driver
[INFO] 2016-04-06 19:53:22,860 org.apache.spark.executor.Executor logInfo - Finished task 1.0 in stage 0.0 (TID 1). 2300 bytes result sent to driver
[INFO] 2016-04-06 19:53:22,865 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 73 ms on localhost (1/2)
[INFO] 2016-04-06 19:53:22,866 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 1.0 in stage 0.0 (TID 1) in 68 ms on localhost (2/2)
[INFO] 2016-04-06 19:53:22,866 org.apache.spark.scheduler.DAGScheduler logInfo - Stage 0 (count at SimpleApp.scala:22) finished in 0.081 s
[INFO] 2016-04-06 19:53:22,866 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2016-04-06 19:53:22,871 org.apache.spark.SparkContext logInfo - Job finished: count at SimpleApp.scala:22, took 0.145318 s
[INFO] 2016-04-06 19:53:22,877 org.apache.spark.SparkContext logInfo - Starting job: count at SimpleApp.scala:23
[INFO] 2016-04-06 19:53:22,878 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (count at SimpleApp.scala:23) with 2 output partitions (allowLocal=false)
[INFO] 2016-04-06 19:53:22,878 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: Stage 1(count at SimpleApp.scala:23)
[INFO] 2016-04-06 19:53:22,878 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2016-04-06 19:53:22,879 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2016-04-06 19:53:22,880 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting Stage 1 (FilteredRDD[3] at filter at SimpleApp.scala:23), which has no missing parents
[INFO] 2016-04-06 19:53:22,881 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(2584) called with curMem=46984, maxMem=2061647216
[INFO] 2016-04-06 19:53:22,882 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 1966.1 MB)
[INFO] 2016-04-06 19:53:22,883 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 2 missing tasks from Stage 1 (FilteredRDD[3] at filter at SimpleApp.scala:23)
[INFO] 2016-04-06 19:53:22,883 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 2 tasks
[INFO] 2016-04-06 19:53:22,886 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 2, localhost, ANY, 1213 bytes)
[INFO] 2016-04-06 19:53:22,887 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 1.0 in stage 1.0 (TID 3, localhost, ANY, 1213 bytes)
[INFO] 2016-04-06 19:53:22,887 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 2)
[INFO] 2016-04-06 19:53:22,887 org.apache.spark.executor.Executor logInfo - Running task 1.0 in stage 1.0 (TID 3)
[INFO] 2016-04-06 19:53:22,891 org.apache.spark.storage.BlockManager logInfo - Found block rdd_1_0 locally
[INFO] 2016-04-06 19:53:22,891 org.apache.spark.storage.BlockManager logInfo - Found block rdd_1_1 locally
[INFO] 2016-04-06 19:53:22,892 org.apache.spark.executor.Executor logInfo - Finished task 1.0 in stage 1.0 (TID 3). 1731 bytes result sent to driver
[INFO] 2016-04-06 19:53:22,892 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 2). 1731 bytes result sent to driver
[INFO] 2016-04-06 19:53:22,894 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 2) in 7 ms on localhost (1/2)
[INFO] 2016-04-06 19:53:22,895 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 1.0 in stage 1.0 (TID 3) in 7 ms on localhost (2/2)
[INFO] 2016-04-06 19:53:22,895 org.apache.spark.scheduler.DAGScheduler logInfo - Stage 1 (count at SimpleApp.scala:23) finished in 0.009 s
[INFO] 2016-04-06 19:53:22,895 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2016-04-06 19:53:22,895 org.apache.spark.SparkContext logInfo - Job finished: count at SimpleApp.scala:23, took 0.017857 s
[INFO] 2016-04-06 21:23:00,291 org.apache.spark.SecurityManager logInfo - Changing view acls to: zhuohuawu,
[INFO] 2016-04-06 21:23:00,294 org.apache.spark.SecurityManager logInfo - Changing modify acls to: zhuohuawu,
[INFO] 2016-04-06 21:23:00,294 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zhuohuawu, ); users with modify permissions: Set(zhuohuawu, )
[INFO] 2016-04-06 21:23:00,545 akka.event.slf4j.Slf4jLogger applyOrElse - Slf4jLogger started
[INFO] 2016-04-06 21:23:00,586 Remoting apply$mcV$sp - Starting remoting
[INFO] 2016-04-06 21:23:00,708 Remoting apply$mcV$sp - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.142:56653]
[INFO] 2016-04-06 21:23:00,710 Remoting apply$mcV$sp - Remoting now listens on addresses: [akka.tcp://sparkDriver@192.168.1.142:56653]
[INFO] 2016-04-06 21:23:00,718 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 56653.
[INFO] 2016-04-06 21:23:00,730 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2016-04-06 21:23:00,737 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2016-04-06 21:23:00,756 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /var/folders/0y/3r3b4yz11pqfyv3xh382szlh0000gn/T/spark-local-20160406212300-8c74
[INFO] 2016-04-06 21:23:00,771 org.apache.spark.util.Utils logInfo - Successfully started service 'Connection manager for block manager' on port 56654.
[INFO] 2016-04-06 21:23:00,772 org.apache.spark.network.ConnectionManager logInfo - Bound socket to port 56654 with id = ConnectionManagerId(192.168.1.142,56654)
[INFO] 2016-04-06 21:23:00,776 org.apache.spark.storage.MemoryStore logInfo - MemoryStore started with capacity 530.3 MB
[INFO] 2016-04-06 21:23:00,780 org.apache.spark.storage.BlockManagerMaster logInfo - Trying to register BlockManager
[INFO] 2016-04-06 21:23:00,781 org.apache.spark.storage.BlockManagerMasterActor logInfo - Registering block manager 192.168.1.142:56654 with 530.3 MB RAM
[INFO] 2016-04-06 21:23:00,783 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager
[INFO] 2016-04-06 21:23:00,791 org.apache.spark.HttpFileServer logInfo - HTTP File server directory is /var/folders/0y/3r3b4yz11pqfyv3xh382szlh0000gn/T/spark-7a26e807-78a7-43bc-b5d6-1b8b083cdaac
[INFO] 2016-04-06 21:23:00,795 org.apache.spark.HttpServer logInfo - Starting HTTP Server
[INFO] 2016-04-06 21:23:00,870 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[INFO] 2016-04-06 21:23:00,886 org.eclipse.jetty.server.AbstractConnector doStart - Started SocketConnector@0.0.0.0:56655
[INFO] 2016-04-06 21:23:00,886 org.apache.spark.util.Utils logInfo - Successfully started service 'HTTP file server' on port 56655.
[INFO] 2016-04-06 21:23:01,067 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[WARN] 2016-04-06 21:23:01,076 org.eclipse.jetty.util.component.AbstractLifeCycle setFailed - FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1446)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1442)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:202)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:224)
	at SimpleApp$.main(SimpleApp.scala:20)
	at SimpleApp.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sbt.Run.invokeMain(Run.scala:67)
	at sbt.Run.run0(Run.scala:61)
	at sbt.Run.sbt$Run$$execute$1(Run.scala:51)
	at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Logger$$anon$4.apply(Logger.scala:85)
	at sbt.TrapExit$App.run(TrapExit.scala:248)
	at java.lang.Thread.run(Thread.java:745)
[WARN] 2016-04-06 21:23:01,086 org.eclipse.jetty.util.component.AbstractLifeCycle setFailed - FAILED org.eclipse.jetty.server.Server@218bcc0f: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1446)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1442)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:202)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:224)
	at SimpleApp$.main(SimpleApp.scala:20)
	at SimpleApp.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sbt.Run.invokeMain(Run.scala:67)
	at sbt.Run.run0(Run.scala:61)
	at sbt.Run.sbt$Run$$execute$1(Run.scala:51)
	at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Logger$$anon$4.apply(Logger.scala:85)
	at sbt.TrapExit$App.run(TrapExit.scala:248)
	at java.lang.Thread.run(Thread.java:745)
[INFO] 2016-04-06 21:23:01,113 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
[INFO] 2016-04-06 21:23:01,114 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
[INFO] 2016-04-06 21:23:01,119 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/,null}
[INFO] 2016-04-06 21:23:01,119 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/static,null}
[INFO] 2016-04-06 21:23:01,119 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
[INFO] 2016-04-06 21:23:01,119 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors,null}
[INFO] 2016-04-06 21:23:01,119 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
[INFO] 2016-04-06 21:23:01,120 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment,null}
[INFO] 2016-04-06 21:23:01,130 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
[INFO] 2016-04-06 21:23:01,130 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
[INFO] 2016-04-06 21:23:01,130 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
[INFO] 2016-04-06 21:23:01,130 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage,null}
[INFO] 2016-04-06 21:23:01,131 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
[INFO] 2016-04-06 21:23:01,131 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
[INFO] 2016-04-06 21:23:01,131 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
[INFO] 2016-04-06 21:23:01,133 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
[INFO] 2016-04-06 21:23:01,134 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
[INFO] 2016-04-06 21:23:01,134 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages,null}
[WARN] 2016-04-06 21:23:01,186 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2016-04-06 21:23:01,187 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[INFO] 2016-04-06 21:23:01,197 org.eclipse.jetty.server.AbstractConnector doStart - Started SelectChannelConnector@0.0.0.0:4041
[INFO] 2016-04-06 21:23:01,198 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2016-04-06 21:23:01,199 org.apache.spark.ui.SparkUI logInfo - Started SparkUI at http://192.168.1.142:4041
[INFO] 2016-04-06 21:23:01,459 org.apache.spark.util.AkkaUtils logInfo - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.1.142:56653/user/HeartbeatReceiver
[INFO] 2016-04-06 21:23:01,539 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(32768) called with curMem=0, maxMem=556038881
[INFO] 2016-04-06 21:23:01,541 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 32.0 KB, free 530.2 MB)
[WARN] 2016-04-06 21:23:01,597 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN] 2016-04-06 21:23:01,598 org.apache.hadoop.io.compress.snappy.LoadSnappy <clinit> - Snappy native library not loaded
[INFO] 2016-04-06 21:23:01,605 org.apache.hadoop.mapred.FileInputFormat listStatus - Total input paths to process : 1
[INFO] 2016-04-06 21:23:01,614 org.apache.spark.SparkContext logInfo - Starting job: count at SimpleApp.scala:22
[INFO] 2016-04-06 21:23:01,624 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (count at SimpleApp.scala:22) with 2 output partitions (allowLocal=false)
[INFO] 2016-04-06 21:23:01,624 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: Stage 0(count at SimpleApp.scala:22)
[INFO] 2016-04-06 21:23:01,625 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2016-04-06 21:23:01,629 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2016-04-06 21:23:01,633 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting Stage 0 (FilteredRDD[2] at filter at SimpleApp.scala:22), which has no missing parents
[INFO] 2016-04-06 21:23:01,653 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(2584) called with curMem=32768, maxMem=556038881
[INFO] 2016-04-06 21:23:01,654 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 2.5 KB, free 530.2 MB)
[INFO] 2016-04-06 21:23:01,664 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 2 missing tasks from Stage 0 (FilteredRDD[2] at filter at SimpleApp.scala:22)
[INFO] 2016-04-06 21:23:01,665 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 2 tasks
[INFO] 2016-04-06 21:23:01,680 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1213 bytes)
[INFO] 2016-04-06 21:23:01,683 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1213 bytes)
[INFO] 2016-04-06 21:23:01,688 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2016-04-06 21:23:01,688 org.apache.spark.executor.Executor logInfo - Running task 1.0 in stage 0.0 (TID 1)
[INFO] 2016-04-06 21:23:01,707 org.apache.spark.CacheManager logInfo - Partition rdd_1_1 not found, computing it
[INFO] 2016-04-06 21:23:01,707 org.apache.spark.CacheManager logInfo - Partition rdd_1_0 not found, computing it
[INFO] 2016-04-06 21:23:01,709 org.apache.spark.rdd.HadoopRDD logInfo - Input split: file:/Users/zhuohuawu/Documents/zw_codes/spark/README.md:1820+1820
[INFO] 2016-04-06 21:23:01,709 org.apache.spark.rdd.HadoopRDD logInfo - Input split: file:/Users/zhuohuawu/Documents/zw_codes/spark/README.md:0+1820
[INFO] 2016-04-06 21:23:01,726 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(5400) called with curMem=35352, maxMem=556038881
[INFO] 2016-04-06 21:23:01,727 org.apache.spark.storage.MemoryStore logInfo - Block rdd_1_1 stored as values in memory (estimated size 5.3 KB, free 530.2 MB)
[INFO] 2016-04-06 21:23:01,727 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(6232) called with curMem=40752, maxMem=556038881
[INFO] 2016-04-06 21:23:01,728 org.apache.spark.storage.MemoryStore logInfo - Block rdd_1_0 stored as values in memory (estimated size 6.1 KB, free 530.2 MB)
[INFO] 2016-04-06 21:23:01,728 org.apache.spark.storage.BlockManagerInfo logInfo - Added rdd_1_1 in memory on 192.168.1.142:56654 (size: 5.3 KB, free: 530.3 MB)
[INFO] 2016-04-06 21:23:01,728 org.apache.spark.storage.BlockManagerInfo logInfo - Added rdd_1_0 in memory on 192.168.1.142:56654 (size: 6.1 KB, free: 530.3 MB)
[INFO] 2016-04-06 21:23:01,728 org.apache.spark.storage.BlockManagerMaster logInfo - Updated info of block rdd_1_1
[INFO] 2016-04-06 21:23:01,728 org.apache.spark.storage.BlockManagerMaster logInfo - Updated info of block rdd_1_0
[INFO] 2016-04-06 21:23:01,749 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 2300 bytes result sent to driver
[INFO] 2016-04-06 21:23:01,749 org.apache.spark.executor.Executor logInfo - Finished task 1.0 in stage 0.0 (TID 1). 2300 bytes result sent to driver
[INFO] 2016-04-06 21:23:01,754 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 1.0 in stage 0.0 (TID 1) in 70 ms on localhost (1/2)
[INFO] 2016-04-06 21:23:01,755 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 77 ms on localhost (2/2)
[INFO] 2016-04-06 21:23:01,755 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2016-04-06 21:23:01,755 org.apache.spark.scheduler.DAGScheduler logInfo - Stage 0 (count at SimpleApp.scala:22) finished in 0.084 s
[INFO] 2016-04-06 21:23:01,760 org.apache.spark.SparkContext logInfo - Job finished: count at SimpleApp.scala:22, took 0.146213 s
[INFO] 2016-04-06 21:23:01,764 org.apache.spark.SparkContext logInfo - Starting job: count at SimpleApp.scala:23
[INFO] 2016-04-06 21:23:01,764 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (count at SimpleApp.scala:23) with 2 output partitions (allowLocal=false)
[INFO] 2016-04-06 21:23:01,765 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: Stage 1(count at SimpleApp.scala:23)
[INFO] 2016-04-06 21:23:01,765 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2016-04-06 21:23:01,766 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2016-04-06 21:23:01,767 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting Stage 1 (FilteredRDD[3] at filter at SimpleApp.scala:23), which has no missing parents
[INFO] 2016-04-06 21:23:01,771 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(2584) called with curMem=46984, maxMem=556038881
[INFO] 2016-04-06 21:23:01,771 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 530.2 MB)
[INFO] 2016-04-06 21:23:01,772 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 2 missing tasks from Stage 1 (FilteredRDD[3] at filter at SimpleApp.scala:23)
[INFO] 2016-04-06 21:23:01,772 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 2 tasks
[INFO] 2016-04-06 21:23:01,777 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 2, localhost, ANY, 1213 bytes)
[INFO] 2016-04-06 21:23:01,778 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 1.0 in stage 1.0 (TID 3, localhost, ANY, 1213 bytes)
[INFO] 2016-04-06 21:23:01,778 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 2)
[INFO] 2016-04-06 21:23:01,778 org.apache.spark.executor.Executor logInfo - Running task 1.0 in stage 1.0 (TID 3)
[INFO] 2016-04-06 21:23:01,781 org.apache.spark.storage.BlockManager logInfo - Found block rdd_1_1 locally
[INFO] 2016-04-06 21:23:01,781 org.apache.spark.storage.BlockManager logInfo - Found block rdd_1_0 locally
[INFO] 2016-04-06 21:23:01,782 org.apache.spark.executor.Executor logInfo - Finished task 1.0 in stage 1.0 (TID 3). 1731 bytes result sent to driver
[INFO] 2016-04-06 21:23:01,784 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 1.0 in stage 1.0 (TID 3) in 7 ms on localhost (1/2)
[INFO] 2016-04-06 21:23:01,782 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 2). 1731 bytes result sent to driver
[INFO] 2016-04-06 21:23:01,790 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 2) in 14 ms on localhost (2/2)
[INFO] 2016-04-06 21:23:01,790 org.apache.spark.scheduler.DAGScheduler logInfo - Stage 1 (count at SimpleApp.scala:23) finished in 0.014 s
[INFO] 2016-04-06 21:23:01,790 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2016-04-06 21:23:01,791 org.apache.spark.SparkContext logInfo - Job finished: count at SimpleApp.scala:23, took 0.026547 s
[ERROR] 2016-04-06 21:23:01,822 org.apache.spark.util.Utils logError - Uncaught exception in thread SparkListenerBus
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:996)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:317)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:48)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)
[ERROR] 2016-04-06 21:23:01,833 org.apache.spark.ContextCleaner logError - Error in cleaning thread
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:136)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:133)
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:65)
[INFO] 2016-04-06 21:23:01,828 org.apache.spark.network.ConnectionManager logInfo - Selector thread was interrupted!
[INFO] 2016-04-06 21:26:03,559 org.apache.spark.SecurityManager logInfo - Changing view acls to: zhuohuawu,
[INFO] 2016-04-06 21:26:03,561 org.apache.spark.SecurityManager logInfo - Changing modify acls to: zhuohuawu,
[INFO] 2016-04-06 21:26:03,561 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zhuohuawu, ); users with modify permissions: Set(zhuohuawu, )
[INFO] 2016-04-06 21:26:03,805 akka.event.slf4j.Slf4jLogger applyOrElse - Slf4jLogger started
[INFO] 2016-04-06 21:26:03,849 Remoting apply$mcV$sp - Starting remoting
[INFO] 2016-04-06 21:26:03,973 Remoting apply$mcV$sp - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.142:56669]
[INFO] 2016-04-06 21:26:03,975 Remoting apply$mcV$sp - Remoting now listens on addresses: [akka.tcp://sparkDriver@192.168.1.142:56669]
[INFO] 2016-04-06 21:26:03,983 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 56669.
[INFO] 2016-04-06 21:26:03,996 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2016-04-06 21:26:04,002 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2016-04-06 21:26:04,021 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /var/folders/0y/3r3b4yz11pqfyv3xh382szlh0000gn/T/spark-local-20160406212604-5c23
[INFO] 2016-04-06 21:26:04,036 org.apache.spark.util.Utils logInfo - Successfully started service 'Connection manager for block manager' on port 56670.
[INFO] 2016-04-06 21:26:04,037 org.apache.spark.network.ConnectionManager logInfo - Bound socket to port 56670 with id = ConnectionManagerId(192.168.1.142,56670)
[INFO] 2016-04-06 21:26:04,042 org.apache.spark.storage.MemoryStore logInfo - MemoryStore started with capacity 530.3 MB
[INFO] 2016-04-06 21:26:04,046 org.apache.spark.storage.BlockManagerMaster logInfo - Trying to register BlockManager
[INFO] 2016-04-06 21:26:04,047 org.apache.spark.storage.BlockManagerMasterActor logInfo - Registering block manager 192.168.1.142:56670 with 530.3 MB RAM
[INFO] 2016-04-06 21:26:04,049 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager
[INFO] 2016-04-06 21:26:04,057 org.apache.spark.HttpFileServer logInfo - HTTP File server directory is /var/folders/0y/3r3b4yz11pqfyv3xh382szlh0000gn/T/spark-73528083-23c1-496a-b2ab-795bc2442124
[INFO] 2016-04-06 21:26:04,061 org.apache.spark.HttpServer logInfo - Starting HTTP Server
[INFO] 2016-04-06 21:26:04,136 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[INFO] 2016-04-06 21:26:04,152 org.eclipse.jetty.server.AbstractConnector doStart - Started SocketConnector@0.0.0.0:56671
[INFO] 2016-04-06 21:26:04,152 org.apache.spark.util.Utils logInfo - Successfully started service 'HTTP file server' on port 56671.
[INFO] 2016-04-06 21:26:04,331 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[WARN] 2016-04-06 21:26:04,339 org.eclipse.jetty.util.component.AbstractLifeCycle setFailed - FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1446)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1442)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:202)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:224)
	at SimpleApp$.main(SimpleApp.scala:20)
	at SimpleApp.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sbt.Run.invokeMain(Run.scala:67)
	at sbt.Run.run0(Run.scala:61)
	at sbt.Run.sbt$Run$$execute$1(Run.scala:51)
	at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Logger$$anon$4.apply(Logger.scala:85)
	at sbt.TrapExit$App.run(TrapExit.scala:248)
	at java.lang.Thread.run(Thread.java:745)
[WARN] 2016-04-06 21:26:04,362 org.eclipse.jetty.util.component.AbstractLifeCycle setFailed - FAILED org.eclipse.jetty.server.Server@44b660c7: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1446)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1442)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:202)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:224)
	at SimpleApp$.main(SimpleApp.scala:20)
	at SimpleApp.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sbt.Run.invokeMain(Run.scala:67)
	at sbt.Run.run0(Run.scala:61)
	at sbt.Run.sbt$Run$$execute$1(Run.scala:51)
	at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Logger$$anon$4.apply(Logger.scala:85)
	at sbt.TrapExit$App.run(TrapExit.scala:248)
	at java.lang.Thread.run(Thread.java:745)
[INFO] 2016-04-06 21:26:04,389 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
[INFO] 2016-04-06 21:26:04,389 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
[INFO] 2016-04-06 21:26:04,405 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/,null}
[INFO] 2016-04-06 21:26:04,406 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/static,null}
[INFO] 2016-04-06 21:26:04,406 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
[INFO] 2016-04-06 21:26:04,406 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors,null}
[INFO] 2016-04-06 21:26:04,406 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
[INFO] 2016-04-06 21:26:04,406 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment,null}
[INFO] 2016-04-06 21:26:04,413 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
[INFO] 2016-04-06 21:26:04,413 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
[INFO] 2016-04-06 21:26:04,413 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
[INFO] 2016-04-06 21:26:04,413 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage,null}
[INFO] 2016-04-06 21:26:04,413 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
[INFO] 2016-04-06 21:26:04,413 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
[INFO] 2016-04-06 21:26:04,414 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
[INFO] 2016-04-06 21:26:04,421 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
[INFO] 2016-04-06 21:26:04,421 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
[INFO] 2016-04-06 21:26:04,421 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages,null}
[WARN] 2016-04-06 21:26:04,473 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2016-04-06 21:26:04,474 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[INFO] 2016-04-06 21:26:04,483 org.eclipse.jetty.server.AbstractConnector doStart - Started SelectChannelConnector@0.0.0.0:4041
[INFO] 2016-04-06 21:26:04,484 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2016-04-06 21:26:04,485 org.apache.spark.ui.SparkUI logInfo - Started SparkUI at http://192.168.1.142:4041
[INFO] 2016-04-06 21:26:04,773 org.apache.spark.util.AkkaUtils logInfo - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.1.142:56669/user/HeartbeatReceiver
[INFO] 2016-04-06 21:26:04,851 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(32768) called with curMem=0, maxMem=556038881
[INFO] 2016-04-06 21:26:04,852 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 32.0 KB, free 530.2 MB)
[WARN] 2016-04-06 21:26:04,909 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN] 2016-04-06 21:26:04,909 org.apache.hadoop.io.compress.snappy.LoadSnappy <clinit> - Snappy native library not loaded
[INFO] 2016-04-06 21:26:04,915 org.apache.hadoop.mapred.FileInputFormat listStatus - Total input paths to process : 1
[INFO] 2016-04-06 21:26:04,925 org.apache.spark.SparkContext logInfo - Starting job: count at SimpleApp.scala:22
[INFO] 2016-04-06 21:26:04,934 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (count at SimpleApp.scala:22) with 2 output partitions (allowLocal=false)
[INFO] 2016-04-06 21:26:04,934 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: Stage 0(count at SimpleApp.scala:22)
[INFO] 2016-04-06 21:26:04,934 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2016-04-06 21:26:04,937 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2016-04-06 21:26:04,940 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting Stage 0 (FilteredRDD[2] at filter at SimpleApp.scala:22), which has no missing parents
[INFO] 2016-04-06 21:26:04,959 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(2584) called with curMem=32768, maxMem=556038881
[INFO] 2016-04-06 21:26:04,959 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 2.5 KB, free 530.2 MB)
[INFO] 2016-04-06 21:26:04,967 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 2 missing tasks from Stage 0 (FilteredRDD[2] at filter at SimpleApp.scala:22)
[INFO] 2016-04-06 21:26:04,967 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 2 tasks
[INFO] 2016-04-06 21:26:04,983 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1213 bytes)
[INFO] 2016-04-06 21:26:04,986 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1213 bytes)
[INFO] 2016-04-06 21:26:04,991 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2016-04-06 21:26:04,991 org.apache.spark.executor.Executor logInfo - Running task 1.0 in stage 0.0 (TID 1)
[INFO] 2016-04-06 21:26:05,010 org.apache.spark.CacheManager logInfo - Partition rdd_1_1 not found, computing it
[INFO] 2016-04-06 21:26:05,010 org.apache.spark.CacheManager logInfo - Partition rdd_1_0 not found, computing it
[INFO] 2016-04-06 21:26:05,013 org.apache.spark.rdd.HadoopRDD logInfo - Input split: file:/Users/zhuohuawu/Documents/zw_codes/spark/README.md:0+1820
[INFO] 2016-04-06 21:26:05,013 org.apache.spark.rdd.HadoopRDD logInfo - Input split: file:/Users/zhuohuawu/Documents/zw_codes/spark/README.md:1820+1820
[INFO] 2016-04-06 21:26:05,031 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(5400) called with curMem=35352, maxMem=556038881
[INFO] 2016-04-06 21:26:05,031 org.apache.spark.storage.MemoryStore logInfo - Block rdd_1_1 stored as values in memory (estimated size 5.3 KB, free 530.2 MB)
[INFO] 2016-04-06 21:26:05,032 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(6232) called with curMem=40752, maxMem=556038881
[INFO] 2016-04-06 21:26:05,032 org.apache.spark.storage.MemoryStore logInfo - Block rdd_1_0 stored as values in memory (estimated size 6.1 KB, free 530.2 MB)
[INFO] 2016-04-06 21:26:05,032 org.apache.spark.storage.BlockManagerInfo logInfo - Added rdd_1_1 in memory on 192.168.1.142:56670 (size: 5.3 KB, free: 530.3 MB)
[INFO] 2016-04-06 21:26:05,033 org.apache.spark.storage.BlockManagerInfo logInfo - Added rdd_1_0 in memory on 192.168.1.142:56670 (size: 6.1 KB, free: 530.3 MB)
[INFO] 2016-04-06 21:26:05,033 org.apache.spark.storage.BlockManagerMaster logInfo - Updated info of block rdd_1_1
[INFO] 2016-04-06 21:26:05,033 org.apache.spark.storage.BlockManagerMaster logInfo - Updated info of block rdd_1_0
[INFO] 2016-04-06 21:26:05,051 org.apache.spark.executor.Executor logInfo - Finished task 1.0 in stage 0.0 (TID 1). 2300 bytes result sent to driver
[INFO] 2016-04-06 21:26:05,051 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 2300 bytes result sent to driver
[INFO] 2016-04-06 21:26:05,057 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 1.0 in stage 0.0 (TID 1) in 69 ms on localhost (1/2)
[INFO] 2016-04-06 21:26:05,057 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 79 ms on localhost (2/2)
[INFO] 2016-04-06 21:26:05,058 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2016-04-06 21:26:05,058 org.apache.spark.scheduler.DAGScheduler logInfo - Stage 0 (count at SimpleApp.scala:22) finished in 0.085 s
[INFO] 2016-04-06 21:26:05,064 org.apache.spark.SparkContext logInfo - Job finished: count at SimpleApp.scala:22, took 0.139502 s
[INFO] 2016-04-06 21:26:05,069 org.apache.spark.SparkContext logInfo - Starting job: count at SimpleApp.scala:23
[INFO] 2016-04-06 21:26:05,070 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (count at SimpleApp.scala:23) with 2 output partitions (allowLocal=false)
[INFO] 2016-04-06 21:26:05,070 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: Stage 1(count at SimpleApp.scala:23)
[INFO] 2016-04-06 21:26:05,070 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2016-04-06 21:26:05,071 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2016-04-06 21:26:05,072 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting Stage 1 (FilteredRDD[3] at filter at SimpleApp.scala:23), which has no missing parents
[INFO] 2016-04-06 21:26:05,073 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(2584) called with curMem=46984, maxMem=556038881
[INFO] 2016-04-06 21:26:05,074 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 530.2 MB)
[INFO] 2016-04-06 21:26:05,075 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 2 missing tasks from Stage 1 (FilteredRDD[3] at filter at SimpleApp.scala:23)
[INFO] 2016-04-06 21:26:05,075 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 2 tasks
[INFO] 2016-04-06 21:26:05,079 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 2, localhost, ANY, 1213 bytes)
[INFO] 2016-04-06 21:26:05,080 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 1.0 in stage 1.0 (TID 3, localhost, ANY, 1213 bytes)
[INFO] 2016-04-06 21:26:05,080 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 2)
[INFO] 2016-04-06 21:26:05,080 org.apache.spark.executor.Executor logInfo - Running task 1.0 in stage 1.0 (TID 3)
[INFO] 2016-04-06 21:26:05,083 org.apache.spark.storage.BlockManager logInfo - Found block rdd_1_0 locally
[INFO] 2016-04-06 21:26:05,084 org.apache.spark.storage.BlockManager logInfo - Found block rdd_1_1 locally
[INFO] 2016-04-06 21:26:05,084 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 2). 1731 bytes result sent to driver
[INFO] 2016-04-06 21:26:05,086 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 2) in 7 ms on localhost (1/2)
[INFO] 2016-04-06 21:26:05,084 org.apache.spark.executor.Executor logInfo - Finished task 1.0 in stage 1.0 (TID 3). 1731 bytes result sent to driver
[INFO] 2016-04-06 21:26:05,091 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 1.0 in stage 1.0 (TID 3) in 12 ms on localhost (2/2)
[INFO] 2016-04-06 21:26:05,091 org.apache.spark.scheduler.DAGScheduler logInfo - Stage 1 (count at SimpleApp.scala:23) finished in 0.013 s
[INFO] 2016-04-06 21:26:05,091 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2016-04-06 21:26:05,091 org.apache.spark.SparkContext logInfo - Job finished: count at SimpleApp.scala:23, took 0.022031 s
[INFO] 2016-04-06 21:26:05,119 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
[INFO] 2016-04-06 21:26:05,119 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
[INFO] 2016-04-06 21:26:05,119 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/,null}
[INFO] 2016-04-06 21:26:05,119 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/static,null}
[INFO] 2016-04-06 21:26:05,119 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
[INFO] 2016-04-06 21:26:05,120 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors,null}
[INFO] 2016-04-06 21:26:05,120 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
[INFO] 2016-04-06 21:26:05,120 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment,null}
[INFO] 2016-04-06 21:26:05,125 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
[INFO] 2016-04-06 21:26:05,125 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
[INFO] 2016-04-06 21:26:05,126 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
[INFO] 2016-04-06 21:26:05,126 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage,null}
[INFO] 2016-04-06 21:26:05,126 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
[INFO] 2016-04-06 21:26:05,126 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
[INFO] 2016-04-06 21:26:05,126 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
[INFO] 2016-04-06 21:26:05,130 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
[INFO] 2016-04-06 21:26:05,130 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
[INFO] 2016-04-06 21:26:05,130 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages,null}
[INFO] 2016-04-06 21:26:05,181 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://192.168.1.142:4041
[INFO] 2016-04-06 21:26:05,183 org.apache.spark.scheduler.DAGScheduler logInfo - Stopping DAGScheduler
[INFO] 2016-04-06 21:26:06,237 org.apache.spark.MapOutputTrackerMasterActor logInfo - MapOutputTrackerActor stopped!
[INFO] 2016-04-06 21:26:06,239 org.apache.spark.network.ConnectionManager logInfo - Selector thread was interrupted!
[INFO] 2016-04-06 21:26:06,241 org.apache.spark.network.ConnectionManager logInfo - ConnectionManager stopped
[INFO] 2016-04-06 21:26:06,243 org.apache.spark.storage.MemoryStore logInfo - MemoryStore cleared
[INFO] 2016-04-06 21:26:06,243 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2016-04-06 21:26:06,244 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2016-04-06 21:26:06,246 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2016-04-06 21:26:06,249 akka.remote.RemoteActorRefProvider$RemotingTerminator apply$mcV$sp - Shutting down remote daemon.
[INFO] 2016-04-06 21:26:06,251 akka.remote.RemoteActorRefProvider$RemotingTerminator apply$mcV$sp - Remote daemon shut down; proceeding with flushing remote transports.
[INFO] 2016-04-06 21:26:06,339 Remoting apply$mcV$sp - Remoting shut down
[INFO] 2016-04-06 21:26:06,339 akka.remote.RemoteActorRefProvider$RemotingTerminator apply$mcV$sp - Remoting shut down.
[INFO] 2016-04-06 21:26:23,297 org.apache.spark.SecurityManager logInfo - Changing view acls to: zhuohuawu,
[INFO] 2016-04-06 21:26:23,299 org.apache.spark.SecurityManager logInfo - Changing modify acls to: zhuohuawu,
[INFO] 2016-04-06 21:26:23,299 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zhuohuawu, ); users with modify permissions: Set(zhuohuawu, )
[INFO] 2016-04-06 21:26:23,539 akka.event.slf4j.Slf4jLogger applyOrElse - Slf4jLogger started
[INFO] 2016-04-06 21:26:23,579 Remoting apply$mcV$sp - Starting remoting
[INFO] 2016-04-06 21:26:23,702 Remoting apply$mcV$sp - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.142:56672]
[INFO] 2016-04-06 21:26:23,704 Remoting apply$mcV$sp - Remoting now listens on addresses: [akka.tcp://sparkDriver@192.168.1.142:56672]
[INFO] 2016-04-06 21:26:23,713 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 56672.
[INFO] 2016-04-06 21:26:23,725 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2016-04-06 21:26:23,733 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2016-04-06 21:26:23,752 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /var/folders/0y/3r3b4yz11pqfyv3xh382szlh0000gn/T/spark-local-20160406212623-650e
[INFO] 2016-04-06 21:26:23,767 org.apache.spark.util.Utils logInfo - Successfully started service 'Connection manager for block manager' on port 56673.
[INFO] 2016-04-06 21:26:23,768 org.apache.spark.network.ConnectionManager logInfo - Bound socket to port 56673 with id = ConnectionManagerId(192.168.1.142,56673)
[INFO] 2016-04-06 21:26:23,771 org.apache.spark.storage.MemoryStore logInfo - MemoryStore started with capacity 530.3 MB
[INFO] 2016-04-06 21:26:23,775 org.apache.spark.storage.BlockManagerMaster logInfo - Trying to register BlockManager
[INFO] 2016-04-06 21:26:23,776 org.apache.spark.storage.BlockManagerMasterActor logInfo - Registering block manager 192.168.1.142:56673 with 530.3 MB RAM
[INFO] 2016-04-06 21:26:23,778 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager
[INFO] 2016-04-06 21:26:23,787 org.apache.spark.HttpFileServer logInfo - HTTP File server directory is /var/folders/0y/3r3b4yz11pqfyv3xh382szlh0000gn/T/spark-bf7fd51d-4f2f-4881-a95d-4022609733f7
[INFO] 2016-04-06 21:26:23,791 org.apache.spark.HttpServer logInfo - Starting HTTP Server
[INFO] 2016-04-06 21:26:23,864 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[INFO] 2016-04-06 21:26:23,879 org.eclipse.jetty.server.AbstractConnector doStart - Started SocketConnector@0.0.0.0:56674
[INFO] 2016-04-06 21:26:23,879 org.apache.spark.util.Utils logInfo - Successfully started service 'HTTP file server' on port 56674.
[INFO] 2016-04-06 21:26:24,055 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[WARN] 2016-04-06 21:26:24,062 org.eclipse.jetty.util.component.AbstractLifeCycle setFailed - FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1446)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1442)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:202)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:224)
	at SimpleApp$.main(SimpleApp.scala:20)
	at SimpleApp.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sbt.Run.invokeMain(Run.scala:67)
	at sbt.Run.run0(Run.scala:61)
	at sbt.Run.sbt$Run$$execute$1(Run.scala:51)
	at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Logger$$anon$4.apply(Logger.scala:85)
	at sbt.TrapExit$App.run(TrapExit.scala:248)
	at java.lang.Thread.run(Thread.java:745)
[WARN] 2016-04-06 21:26:24,076 org.eclipse.jetty.util.component.AbstractLifeCycle setFailed - FAILED org.eclipse.jetty.server.Server@1ef5a08c: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.ui.JettyUtils$$anonfun$3.apply(JettyUtils.scala:202)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1446)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1442)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:202)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:224)
	at SimpleApp$.main(SimpleApp.scala:20)
	at SimpleApp.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sbt.Run.invokeMain(Run.scala:67)
	at sbt.Run.run0(Run.scala:61)
	at sbt.Run.sbt$Run$$execute$1(Run.scala:51)
	at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
	at sbt.Logger$$anon$4.apply(Logger.scala:85)
	at sbt.TrapExit$App.run(TrapExit.scala:248)
	at java.lang.Thread.run(Thread.java:745)
[INFO] 2016-04-06 21:26:24,096 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
[INFO] 2016-04-06 21:26:24,097 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
[INFO] 2016-04-06 21:26:24,113 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/,null}
[INFO] 2016-04-06 21:26:24,113 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/static,null}
[INFO] 2016-04-06 21:26:24,113 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
[INFO] 2016-04-06 21:26:24,113 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors,null}
[INFO] 2016-04-06 21:26:24,113 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
[INFO] 2016-04-06 21:26:24,114 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment,null}
[INFO] 2016-04-06 21:26:24,124 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
[INFO] 2016-04-06 21:26:24,124 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
[INFO] 2016-04-06 21:26:24,125 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
[INFO] 2016-04-06 21:26:24,125 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage,null}
[INFO] 2016-04-06 21:26:24,125 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
[INFO] 2016-04-06 21:26:24,125 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
[INFO] 2016-04-06 21:26:24,125 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
[INFO] 2016-04-06 21:26:24,131 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
[INFO] 2016-04-06 21:26:24,131 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
[INFO] 2016-04-06 21:26:24,131 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages,null}
[WARN] 2016-04-06 21:26:24,185 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2016-04-06 21:26:24,186 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[INFO] 2016-04-06 21:26:24,198 org.eclipse.jetty.server.AbstractConnector doStart - Started SelectChannelConnector@0.0.0.0:4041
[INFO] 2016-04-06 21:26:24,229 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2016-04-06 21:26:24,232 org.apache.spark.ui.SparkUI logInfo - Started SparkUI at http://192.168.1.142:4041
[INFO] 2016-04-06 21:26:24,819 org.apache.spark.util.AkkaUtils logInfo - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.1.142:56672/user/HeartbeatReceiver
[INFO] 2016-04-06 21:26:24,902 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(32768) called with curMem=0, maxMem=556038881
[INFO] 2016-04-06 21:26:24,904 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 32.0 KB, free 530.2 MB)
[WARN] 2016-04-06 21:26:24,960 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN] 2016-04-06 21:26:24,960 org.apache.hadoop.io.compress.snappy.LoadSnappy <clinit> - Snappy native library not loaded
[INFO] 2016-04-06 21:26:24,967 org.apache.hadoop.mapred.FileInputFormat listStatus - Total input paths to process : 1
[INFO] 2016-04-06 21:26:24,975 org.apache.spark.SparkContext logInfo - Starting job: count at SimpleApp.scala:22
[INFO] 2016-04-06 21:26:24,986 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (count at SimpleApp.scala:22) with 2 output partitions (allowLocal=false)
[INFO] 2016-04-06 21:26:24,986 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: Stage 0(count at SimpleApp.scala:22)
[INFO] 2016-04-06 21:26:24,987 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2016-04-06 21:26:24,990 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2016-04-06 21:26:24,993 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting Stage 0 (FilteredRDD[2] at filter at SimpleApp.scala:22), which has no missing parents
[INFO] 2016-04-06 21:26:25,015 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(2584) called with curMem=32768, maxMem=556038881
[INFO] 2016-04-06 21:26:25,015 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 2.5 KB, free 530.2 MB)
[INFO] 2016-04-06 21:26:25,023 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 2 missing tasks from Stage 0 (FilteredRDD[2] at filter at SimpleApp.scala:22)
[INFO] 2016-04-06 21:26:25,024 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 2 tasks
[INFO] 2016-04-06 21:26:25,039 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1213 bytes)
[INFO] 2016-04-06 21:26:25,042 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1213 bytes)
[INFO] 2016-04-06 21:26:25,047 org.apache.spark.executor.Executor logInfo - Running task 1.0 in stage 0.0 (TID 1)
[INFO] 2016-04-06 21:26:25,047 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2016-04-06 21:26:25,066 org.apache.spark.CacheManager logInfo - Partition rdd_1_0 not found, computing it
[INFO] 2016-04-06 21:26:25,066 org.apache.spark.CacheManager logInfo - Partition rdd_1_1 not found, computing it
[INFO] 2016-04-06 21:26:25,069 org.apache.spark.rdd.HadoopRDD logInfo - Input split: file:/Users/zhuohuawu/Documents/zw_codes/spark/README.md:1820+1820
[INFO] 2016-04-06 21:26:25,069 org.apache.spark.rdd.HadoopRDD logInfo - Input split: file:/Users/zhuohuawu/Documents/zw_codes/spark/README.md:0+1820
[INFO] 2016-04-06 21:26:25,088 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(5400) called with curMem=35352, maxMem=556038881
[INFO] 2016-04-06 21:26:25,088 org.apache.spark.storage.MemoryStore logInfo - Block rdd_1_1 stored as values in memory (estimated size 5.3 KB, free 530.2 MB)
[INFO] 2016-04-06 21:26:25,088 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(6232) called with curMem=40752, maxMem=556038881
[INFO] 2016-04-06 21:26:25,089 org.apache.spark.storage.MemoryStore logInfo - Block rdd_1_0 stored as values in memory (estimated size 6.1 KB, free 530.2 MB)
[INFO] 2016-04-06 21:26:25,089 org.apache.spark.storage.BlockManagerInfo logInfo - Added rdd_1_1 in memory on 192.168.1.142:56673 (size: 5.3 KB, free: 530.3 MB)
[INFO] 2016-04-06 21:26:25,090 org.apache.spark.storage.BlockManagerInfo logInfo - Added rdd_1_0 in memory on 192.168.1.142:56673 (size: 6.1 KB, free: 530.3 MB)
[INFO] 2016-04-06 21:26:25,090 org.apache.spark.storage.BlockManagerMaster logInfo - Updated info of block rdd_1_1
[INFO] 2016-04-06 21:26:25,090 org.apache.spark.storage.BlockManagerMaster logInfo - Updated info of block rdd_1_0
[INFO] 2016-04-06 21:26:25,105 org.apache.spark.executor.Executor logInfo - Finished task 1.0 in stage 0.0 (TID 1). 2300 bytes result sent to driver
[INFO] 2016-04-06 21:26:25,106 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 2300 bytes result sent to driver
[INFO] 2016-04-06 21:26:25,111 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 1.0 in stage 0.0 (TID 1) in 67 ms on localhost (1/2)
[INFO] 2016-04-06 21:26:25,112 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 76 ms on localhost (2/2)
[INFO] 2016-04-06 21:26:25,112 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2016-04-06 21:26:25,113 org.apache.spark.scheduler.DAGScheduler logInfo - Stage 0 (count at SimpleApp.scala:22) finished in 0.083 s
[INFO] 2016-04-06 21:26:25,118 org.apache.spark.SparkContext logInfo - Job finished: count at SimpleApp.scala:22, took 0.142772 s
[INFO] 2016-04-06 21:26:25,122 org.apache.spark.SparkContext logInfo - Starting job: count at SimpleApp.scala:23
[INFO] 2016-04-06 21:26:25,123 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (count at SimpleApp.scala:23) with 2 output partitions (allowLocal=false)
[INFO] 2016-04-06 21:26:25,123 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: Stage 1(count at SimpleApp.scala:23)
[INFO] 2016-04-06 21:26:25,123 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2016-04-06 21:26:25,124 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2016-04-06 21:26:25,125 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting Stage 1 (FilteredRDD[3] at filter at SimpleApp.scala:23), which has no missing parents
[INFO] 2016-04-06 21:26:25,126 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(2584) called with curMem=46984, maxMem=556038881
[INFO] 2016-04-06 21:26:25,130 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 530.2 MB)
[INFO] 2016-04-06 21:26:25,131 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 2 missing tasks from Stage 1 (FilteredRDD[3] at filter at SimpleApp.scala:23)
[INFO] 2016-04-06 21:26:25,131 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 2 tasks
[INFO] 2016-04-06 21:26:25,136 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 2, localhost, ANY, 1213 bytes)
[INFO] 2016-04-06 21:26:25,137 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 1.0 in stage 1.0 (TID 3, localhost, ANY, 1213 bytes)
[INFO] 2016-04-06 21:26:25,137 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 2)
[INFO] 2016-04-06 21:26:25,137 org.apache.spark.executor.Executor logInfo - Running task 1.0 in stage 1.0 (TID 3)
[INFO] 2016-04-06 21:26:25,140 org.apache.spark.storage.BlockManager logInfo - Found block rdd_1_0 locally
[INFO] 2016-04-06 21:26:25,141 org.apache.spark.storage.BlockManager logInfo - Found block rdd_1_1 locally
[INFO] 2016-04-06 21:26:25,141 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 2). 1731 bytes result sent to driver
[INFO] 2016-04-06 21:26:25,143 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 2) in 8 ms on localhost (1/2)
[INFO] 2016-04-06 21:26:25,141 org.apache.spark.executor.Executor logInfo - Finished task 1.0 in stage 1.0 (TID 3). 1731 bytes result sent to driver
[INFO] 2016-04-06 21:26:25,150 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 1.0 in stage 1.0 (TID 3) in 14 ms on localhost (2/2)
[INFO] 2016-04-06 21:26:25,150 org.apache.spark.scheduler.DAGScheduler logInfo - Stage 1 (count at SimpleApp.scala:23) finished in 0.015 s
[INFO] 2016-04-06 21:26:25,150 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2016-04-06 21:26:25,151 org.apache.spark.SparkContext logInfo - Job finished: count at SimpleApp.scala:23, took 0.028412 s
[INFO] 2016-04-06 21:26:25,181 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
[INFO] 2016-04-06 21:26:25,181 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
[INFO] 2016-04-06 21:26:25,181 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/,null}
[INFO] 2016-04-06 21:26:25,181 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/static,null}
[INFO] 2016-04-06 21:26:25,182 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
[INFO] 2016-04-06 21:26:25,182 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors,null}
[INFO] 2016-04-06 21:26:25,182 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
[INFO] 2016-04-06 21:26:25,182 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment,null}
[INFO] 2016-04-06 21:26:25,186 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
[INFO] 2016-04-06 21:26:25,186 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
[INFO] 2016-04-06 21:26:25,186 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
[INFO] 2016-04-06 21:26:25,186 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage,null}
[INFO] 2016-04-06 21:26:25,186 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
[INFO] 2016-04-06 21:26:25,186 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
[INFO] 2016-04-06 21:26:25,187 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
[INFO] 2016-04-06 21:26:25,190 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
[INFO] 2016-04-06 21:26:25,191 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
[INFO] 2016-04-06 21:26:25,191 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages,null}
[INFO] 2016-04-06 21:26:25,243 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://192.168.1.142:4041
[INFO] 2016-04-06 21:26:25,244 org.apache.spark.scheduler.DAGScheduler logInfo - Stopping DAGScheduler
[INFO] 2016-04-06 21:26:26,301 org.apache.spark.MapOutputTrackerMasterActor logInfo - MapOutputTrackerActor stopped!
[INFO] 2016-04-06 21:26:26,303 org.apache.spark.network.ConnectionManager logInfo - Selector thread was interrupted!
[INFO] 2016-04-06 21:26:26,304 org.apache.spark.network.ConnectionManager logInfo - ConnectionManager stopped
[INFO] 2016-04-06 21:26:26,305 org.apache.spark.storage.MemoryStore logInfo - MemoryStore cleared
[INFO] 2016-04-06 21:26:26,306 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2016-04-06 21:26:26,306 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2016-04-06 21:26:26,308 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2016-04-06 21:26:26,310 akka.remote.RemoteActorRefProvider$RemotingTerminator apply$mcV$sp - Shutting down remote daemon.
[INFO] 2016-04-06 21:26:26,311 akka.remote.RemoteActorRefProvider$RemotingTerminator apply$mcV$sp - Remote daemon shut down; proceeding with flushing remote transports.
[INFO] 2016-04-06 21:27:00,197 org.apache.spark.SecurityManager logInfo - Changing view acls to: zhuohuawu,
[INFO] 2016-04-06 21:27:00,200 org.apache.spark.SecurityManager logInfo - Changing modify acls to: zhuohuawu,
[INFO] 2016-04-06 21:27:00,200 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zhuohuawu, ); users with modify permissions: Set(zhuohuawu, )
[INFO] 2016-04-06 21:27:00,449 akka.event.slf4j.Slf4jLogger applyOrElse - Slf4jLogger started
[INFO] 2016-04-06 21:27:00,491 Remoting apply$mcV$sp - Starting remoting
[INFO] 2016-04-06 21:27:00,615 Remoting apply$mcV$sp - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.142:56675]
[INFO] 2016-04-06 21:27:00,617 Remoting apply$mcV$sp - Remoting now listens on addresses: [akka.tcp://sparkDriver@192.168.1.142:56675]
[INFO] 2016-04-06 21:27:00,625 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 56675.
[INFO] 2016-04-06 21:27:00,637 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2016-04-06 21:27:00,643 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2016-04-06 21:27:00,662 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /var/folders/0y/3r3b4yz11pqfyv3xh382szlh0000gn/T/spark-local-20160406212700-01a4
[INFO] 2016-04-06 21:27:00,677 org.apache.spark.util.Utils logInfo - Successfully started service 'Connection manager for block manager' on port 56676.
[INFO] 2016-04-06 21:27:00,678 org.apache.spark.network.ConnectionManager logInfo - Bound socket to port 56676 with id = ConnectionManagerId(192.168.1.142,56676)
[INFO] 2016-04-06 21:27:00,681 org.apache.spark.storage.MemoryStore logInfo - MemoryStore started with capacity 530.3 MB
[INFO] 2016-04-06 21:27:00,685 org.apache.spark.storage.BlockManagerMaster logInfo - Trying to register BlockManager
[INFO] 2016-04-06 21:27:00,686 org.apache.spark.storage.BlockManagerMasterActor logInfo - Registering block manager 192.168.1.142:56676 with 530.3 MB RAM
[INFO] 2016-04-06 21:27:00,688 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager
[INFO] 2016-04-06 21:27:00,697 org.apache.spark.HttpFileServer logInfo - HTTP File server directory is /var/folders/0y/3r3b4yz11pqfyv3xh382szlh0000gn/T/spark-4b3e08d6-1693-4fe6-a762-35aaa871bc88
[INFO] 2016-04-06 21:27:00,701 org.apache.spark.HttpServer logInfo - Starting HTTP Server
[INFO] 2016-04-06 21:27:00,775 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[INFO] 2016-04-06 21:27:00,791 org.eclipse.jetty.server.AbstractConnector doStart - Started SocketConnector@0.0.0.0:56677
[INFO] 2016-04-06 21:27:00,791 org.apache.spark.util.Utils logInfo - Successfully started service 'HTTP file server' on port 56677.
[INFO] 2016-04-06 21:27:00,966 org.eclipse.jetty.server.Server doStart - jetty-8.1.14.v20131031
[INFO] 2016-04-06 21:27:00,977 org.eclipse.jetty.server.AbstractConnector doStart - Started SelectChannelConnector@0.0.0.0:4040
[INFO] 2016-04-06 21:27:00,977 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2016-04-06 21:27:00,979 org.apache.spark.ui.SparkUI logInfo - Started SparkUI at http://192.168.1.142:4040
[INFO] 2016-04-06 21:27:03,335 org.apache.spark.util.AkkaUtils logInfo - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.1.142:56675/user/HeartbeatReceiver
[INFO] 2016-04-06 21:27:03,424 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(32768) called with curMem=0, maxMem=556038881
[INFO] 2016-04-06 21:27:03,425 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 32.0 KB, free 530.2 MB)
[WARN] 2016-04-06 21:27:03,484 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN] 2016-04-06 21:27:03,484 org.apache.hadoop.io.compress.snappy.LoadSnappy <clinit> - Snappy native library not loaded
[INFO] 2016-04-06 21:27:03,490 org.apache.hadoop.mapred.FileInputFormat listStatus - Total input paths to process : 1
[INFO] 2016-04-06 21:27:03,499 org.apache.spark.SparkContext logInfo - Starting job: count at SimpleApp.scala:22
[INFO] 2016-04-06 21:27:03,510 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (count at SimpleApp.scala:22) with 2 output partitions (allowLocal=false)
[INFO] 2016-04-06 21:27:03,510 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: Stage 0(count at SimpleApp.scala:22)
[INFO] 2016-04-06 21:27:03,510 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2016-04-06 21:27:03,514 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2016-04-06 21:27:03,516 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting Stage 0 (FilteredRDD[2] at filter at SimpleApp.scala:22), which has no missing parents
[INFO] 2016-04-06 21:27:03,536 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(2584) called with curMem=32768, maxMem=556038881
[INFO] 2016-04-06 21:27:03,537 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 2.5 KB, free 530.2 MB)
[INFO] 2016-04-06 21:27:03,544 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 2 missing tasks from Stage 0 (FilteredRDD[2] at filter at SimpleApp.scala:22)
[INFO] 2016-04-06 21:27:03,545 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 2 tasks
[INFO] 2016-04-06 21:27:03,562 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1213 bytes)
[INFO] 2016-04-06 21:27:03,564 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1213 bytes)
[INFO] 2016-04-06 21:27:03,569 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2016-04-06 21:27:03,569 org.apache.spark.executor.Executor logInfo - Running task 1.0 in stage 0.0 (TID 1)
[INFO] 2016-04-06 21:27:03,588 org.apache.spark.CacheManager logInfo - Partition rdd_1_0 not found, computing it
[INFO] 2016-04-06 21:27:03,588 org.apache.spark.CacheManager logInfo - Partition rdd_1_1 not found, computing it
[INFO] 2016-04-06 21:27:03,592 org.apache.spark.rdd.HadoopRDD logInfo - Input split: file:/Users/zhuohuawu/Documents/zw_codes/spark/README.md:1820+1820
[INFO] 2016-04-06 21:27:03,592 org.apache.spark.rdd.HadoopRDD logInfo - Input split: file:/Users/zhuohuawu/Documents/zw_codes/spark/README.md:0+1820
[INFO] 2016-04-06 21:27:03,609 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(5400) called with curMem=35352, maxMem=556038881
[INFO] 2016-04-06 21:27:03,610 org.apache.spark.storage.MemoryStore logInfo - Block rdd_1_1 stored as values in memory (estimated size 5.3 KB, free 530.2 MB)
[INFO] 2016-04-06 21:27:03,611 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(6232) called with curMem=40752, maxMem=556038881
[INFO] 2016-04-06 21:27:03,611 org.apache.spark.storage.BlockManagerInfo logInfo - Added rdd_1_1 in memory on 192.168.1.142:56676 (size: 5.3 KB, free: 530.3 MB)
[INFO] 2016-04-06 21:27:03,611 org.apache.spark.storage.MemoryStore logInfo - Block rdd_1_0 stored as values in memory (estimated size 6.1 KB, free 530.2 MB)
[INFO] 2016-04-06 21:27:03,611 org.apache.spark.storage.BlockManagerInfo logInfo - Added rdd_1_0 in memory on 192.168.1.142:56676 (size: 6.1 KB, free: 530.3 MB)
[INFO] 2016-04-06 21:27:03,611 org.apache.spark.storage.BlockManagerMaster logInfo - Updated info of block rdd_1_1
[INFO] 2016-04-06 21:27:03,612 org.apache.spark.storage.BlockManagerMaster logInfo - Updated info of block rdd_1_0
[INFO] 2016-04-06 21:27:03,628 org.apache.spark.executor.Executor logInfo - Finished task 1.0 in stage 0.0 (TID 1). 2300 bytes result sent to driver
[INFO] 2016-04-06 21:27:03,628 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 2300 bytes result sent to driver
[INFO] 2016-04-06 21:27:03,633 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 1.0 in stage 0.0 (TID 1) in 68 ms on localhost (1/2)
[INFO] 2016-04-06 21:27:03,634 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 75 ms on localhost (2/2)
[INFO] 2016-04-06 21:27:03,634 org.apache.spark.scheduler.DAGScheduler logInfo - Stage 0 (count at SimpleApp.scala:22) finished in 0.084 s
[INFO] 2016-04-06 21:27:03,634 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2016-04-06 21:27:03,640 org.apache.spark.SparkContext logInfo - Job finished: count at SimpleApp.scala:22, took 0.140656 s
[INFO] 2016-04-06 21:27:03,644 org.apache.spark.SparkContext logInfo - Starting job: count at SimpleApp.scala:23
[INFO] 2016-04-06 21:27:03,645 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (count at SimpleApp.scala:23) with 2 output partitions (allowLocal=false)
[INFO] 2016-04-06 21:27:03,645 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: Stage 1(count at SimpleApp.scala:23)
[INFO] 2016-04-06 21:27:03,645 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2016-04-06 21:27:03,646 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2016-04-06 21:27:03,647 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting Stage 1 (FilteredRDD[3] at filter at SimpleApp.scala:23), which has no missing parents
[INFO] 2016-04-06 21:27:03,649 org.apache.spark.storage.MemoryStore logInfo - ensureFreeSpace(2584) called with curMem=46984, maxMem=556038881
[INFO] 2016-04-06 21:27:03,649 org.apache.spark.storage.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 530.2 MB)
[INFO] 2016-04-06 21:27:03,650 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 2 missing tasks from Stage 1 (FilteredRDD[3] at filter at SimpleApp.scala:23)
[INFO] 2016-04-06 21:27:03,650 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 2 tasks
[INFO] 2016-04-06 21:27:03,654 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 2, localhost, ANY, 1213 bytes)
[INFO] 2016-04-06 21:27:03,655 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 1.0 in stage 1.0 (TID 3, localhost, ANY, 1213 bytes)
[INFO] 2016-04-06 21:27:03,656 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 2)
[INFO] 2016-04-06 21:27:03,656 org.apache.spark.executor.Executor logInfo - Running task 1.0 in stage 1.0 (TID 3)
[INFO] 2016-04-06 21:27:03,659 org.apache.spark.storage.BlockManager logInfo - Found block rdd_1_0 locally
[INFO] 2016-04-06 21:27:03,659 org.apache.spark.storage.BlockManager logInfo - Found block rdd_1_1 locally
[INFO] 2016-04-06 21:27:03,660 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 2). 1731 bytes result sent to driver
[INFO] 2016-04-06 21:27:03,660 org.apache.spark.executor.Executor logInfo - Finished task 1.0 in stage 1.0 (TID 3). 1731 bytes result sent to driver
[INFO] 2016-04-06 21:27:03,662 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 1.0 in stage 1.0 (TID 3) in 7 ms on localhost (1/2)
[INFO] 2016-04-06 21:27:03,667 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 2) in 12 ms on localhost (2/2)
[INFO] 2016-04-06 21:27:03,667 org.apache.spark.scheduler.DAGScheduler logInfo - Stage 1 (count at SimpleApp.scala:23) finished in 0.013 s
[INFO] 2016-04-06 21:27:03,667 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2016-04-06 21:27:03,667 org.apache.spark.SparkContext logInfo - Job finished: count at SimpleApp.scala:23, took 0.022789 s
[INFO] 2016-04-06 21:27:03,692 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
[INFO] 2016-04-06 21:27:03,692 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
[INFO] 2016-04-06 21:27:03,692 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/,null}
[INFO] 2016-04-06 21:27:03,692 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/static,null}
[INFO] 2016-04-06 21:27:03,693 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
[INFO] 2016-04-06 21:27:03,693 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/executors,null}
[INFO] 2016-04-06 21:27:03,693 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
[INFO] 2016-04-06 21:27:03,693 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/environment,null}
[INFO] 2016-04-06 21:27:03,698 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
[INFO] 2016-04-06 21:27:03,698 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
[INFO] 2016-04-06 21:27:03,698 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
[INFO] 2016-04-06 21:27:03,698 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/storage,null}
[INFO] 2016-04-06 21:27:03,698 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
[INFO] 2016-04-06 21:27:03,699 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
[INFO] 2016-04-06 21:27:03,699 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
[INFO] 2016-04-06 21:27:03,706 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
[INFO] 2016-04-06 21:27:03,707 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
[INFO] 2016-04-06 21:27:03,707 org.eclipse.jetty.server.handler.ContextHandler doStop - stopped o.e.j.s.ServletContextHandler{/stages,null}
[INFO] 2016-04-06 21:27:03,759 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://192.168.1.142:4040
[INFO] 2016-04-06 21:27:03,761 org.apache.spark.scheduler.DAGScheduler logInfo - Stopping DAGScheduler
[INFO] 2016-04-06 21:27:04,816 org.apache.spark.MapOutputTrackerMasterActor logInfo - MapOutputTrackerActor stopped!
[INFO] 2016-04-06 21:27:04,818 org.apache.spark.network.ConnectionManager logInfo - Selector thread was interrupted!
[INFO] 2016-04-06 21:27:04,819 org.apache.spark.network.ConnectionManager logInfo - ConnectionManager stopped
[INFO] 2016-04-06 21:27:04,821 org.apache.spark.storage.MemoryStore logInfo - MemoryStore cleared
[INFO] 2016-04-06 21:27:04,822 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2016-04-06 21:27:04,822 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2016-04-06 21:27:04,824 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2016-04-06 21:27:04,826 akka.remote.RemoteActorRefProvider$RemotingTerminator apply$mcV$sp - Shutting down remote daemon.
[INFO] 2016-04-06 21:27:04,827 akka.remote.RemoteActorRefProvider$RemotingTerminator apply$mcV$sp - Remote daemon shut down; proceeding with flushing remote transports.
